@article {watkins_qlearning_1992,
   author = {Watkins, Christopher J. C. H. and Dayan, Peter},
   title = {Q-learning},
   journal = {Machine Learning},
   publisher = {Springer Netherlands},
   issn = {0885-6125},
   keyword = {Computer Science},
   pages = {279-292},
   volume = {8},
   issue = {3},
   year = {1992}
}

@article{jaakkola_convergence_1994,
	title = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},
	volume = {6},
	issn = {0899-7667},
	doi = {10.1162/neco.1994.6.6.1185},
	abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the {TD($\lambda$)} algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming {(DP).} In this paper we provide a rigorous proof of convergence of these {DP-based} learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both {TD(Î»)} and Q-learning belong.},
	number = {6},
	journal = {Neural Computation},
	author = {Tommi Jaakkola and Michael I. Jordan and Satinder P. Singh},
	month = nov,
	year = {1994},
	pages = {1185--1201},
	publisher = {MIT Press}
}

@book{sutton_rl_1998,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Reinforcement Learning: An Introduction},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
